{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a474ee7-1fb3-46a2-ac9c-29f58f9ef254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57f31eb-5261-47f3-a029-b1f61d118d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /home/soham/anaconda3/envs/jupyter_env/lib/python3.10/site-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/soham/anaconda3/envs/jupyter_env/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/soham/anaconda3/envs/jupyter_env/lib/python3.10/site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/soham/anaconda3/envs/jupyter_env/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /home/soham/anaconda3/envs/jupyter_env/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-2.0.1-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.76.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/soham/anaconda3/envs/jupyter_env/lib/python3.10/site-packages (from tensorflow) (2.2.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.15.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/soham/anaconda3/envs/jupyter_env/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/soham/anaconda3/envs/jupyter_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/soham/anaconda3/envs/jupyter_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/soham/anaconda3/envs/jupyter_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/soham/anaconda3/envs/jupyter_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in /home/soham/anaconda3/envs/jupyter_env/lib/python3.10/site-packages (from tensorboard~=2.20.0->tensorflow) (11.1.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/soham/anaconda3/envs/jupyter_env/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/soham/anaconda3/envs/jupyter_env/lib/python3.10/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.4/620.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:10\u001b[0mm\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.76.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:02\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.15.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "Downloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Downloading ml_dtypes-0.5.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hUsing cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-2.0.1-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (113 kB)\n",
      "Downloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (387 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt_einsum, ml_dtypes, mdurl, markdown, h5py, grpcio, google_pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.9.23 gast-0.6.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 keras-3.12.0 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 mdurl-0.1.2 ml_dtypes-0.5.3 namex-0.1.0 opt_einsum-3.4.0 optree-0.17.0 protobuf-6.33.0 rich-14.2.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.2.0 werkzeug-3.1.3 wrapt-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "08eae6f6-9f0d-4ec0-a7f2-179bc5a2dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2d0a7-e2a7-46c5-8bd8-4f7c286097bd",
   "metadata": {},
   "source": [
    "Assignment 3¶\n",
    "\n",
    "Given a bank customer, build a neural network-based classifier that can determine whether they will leave or not in the next 6 months. Dataset Description: The case study is from an open-source dataset from Kaggle. The dataset contains 10,000 sample points with 14 distinct features such as CustomerId, CreditScore, Geography, Gender, Age, Tenure, Balance, etc.\n",
    "\n",
    "Link to the Kaggle project: https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling\n",
    "\n",
    "Perform following steps:\n",
    "\n",
    "    Read the dataset.\n",
    "    Distinguish the feature and target set and divide the data set into training and test sets.\n",
    "    Normalize the train and test data.\n",
    "    Initialize and build the model. Identify the points of improvement and implement the same.\n",
    "    Print the accuracy score and confusion matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63156d96-9d03-433c-a529-4acf2d3fa4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"3-Churn_Modelling.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "943245ec-bcc5-4333-822d-9afd37594763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "843a7ae2-7feb-493b-841b-92d39355e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['RowNumber', 'CustomerId'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c9882e76-9a16-4e21-a419-ab94a6cd062d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Surname  CreditScore Geography  Gender  Age  Tenure    Balance  \\\n",
       "0  Hargrave          619    France  Female   42       2       0.00   \n",
       "1      Hill          608     Spain  Female   41       1   83807.86   \n",
       "2      Onio          502    France  Female   42       8  159660.80   \n",
       "3      Boni          699    France  Female   39       1       0.00   \n",
       "4  Mitchell          850     Spain  Female   43       2  125510.82   \n",
       "\n",
       "   NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n",
       "0              1          1               1        101348.88       1  \n",
       "1              1          0               1        112542.58       0  \n",
       "2              3          1               0        113931.57       1  \n",
       "3              2          0               0         93826.63       0  \n",
       "4              1          1               1         79084.10       0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "17ffa374-95f5-4b4b-978a-fcf85ec40f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_encoder = LabelEncoder()\n",
    "\n",
    "df['Surname'] = lable_encoder.fit_transform(df['Surname'])\n",
    "df['Geography'] =lable_encoder.fit_transform(df['Geography'])\n",
    "df['Gender'] =lable_encoder.fit_transform(df['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4aff85bf-b221-4fe2-b4eb-671e74b9d90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Surname              int64\n",
       "CreditScore          int64\n",
       "Geography            int64\n",
       "Gender               int64\n",
       "Age                  int64\n",
       "Tenure               int64\n",
       "Balance            float64\n",
       "NumOfProducts        int64\n",
       "HasCrCard            int64\n",
       "IsActiveMember       int64\n",
       "EstimatedSalary    float64\n",
       "Exited               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "84b4a4bc-3ece-4582-94cf-d4e1af4d59e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features and the target variables\n",
    "X = df.drop('Exited', axis = 1) # Features\n",
    "Y = df['Exited'] # Target\n",
    "\n",
    "# Split the dataset into training and testing set\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c12ef9-b16f-4573-b120-5f1093614bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28c38ffb-9be2-4d55-84f3-c739bbf6baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dataset\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(x_train)\n",
    "X_test = scaler.transform(x_test)\n",
    "# we use only transform on xtest as it uses the same mean and SD learned from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "49d3186b-bbb1-4a41-859c-f9fce34bbc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of normalized training data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.361279</td>\n",
       "      <td>0.336963</td>\n",
       "      <td>-0.900813</td>\n",
       "      <td>0.916475</td>\n",
       "      <td>-0.086218</td>\n",
       "      <td>0.680987</td>\n",
       "      <td>0.534294</td>\n",
       "      <td>0.807162</td>\n",
       "      <td>0.646675</td>\n",
       "      <td>-1.024808</td>\n",
       "      <td>0.041673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.165571</td>\n",
       "      <td>0.616810</td>\n",
       "      <td>-0.900813</td>\n",
       "      <td>-1.091137</td>\n",
       "      <td>-0.277681</td>\n",
       "      <td>-0.354341</td>\n",
       "      <td>0.642170</td>\n",
       "      <td>-0.913407</td>\n",
       "      <td>0.646675</td>\n",
       "      <td>-1.024808</td>\n",
       "      <td>-0.724481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.129260</td>\n",
       "      <td>0.482069</td>\n",
       "      <td>-0.900813</td>\n",
       "      <td>-1.091137</td>\n",
       "      <td>-0.564876</td>\n",
       "      <td>-1.389668</td>\n",
       "      <td>0.182146</td>\n",
       "      <td>-0.913407</td>\n",
       "      <td>0.646675</td>\n",
       "      <td>-1.024808</td>\n",
       "      <td>1.256317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.739638</td>\n",
       "      <td>0.834469</td>\n",
       "      <td>0.304691</td>\n",
       "      <td>0.916475</td>\n",
       "      <td>-0.373413</td>\n",
       "      <td>-1.044559</td>\n",
       "      <td>0.830709</td>\n",
       "      <td>0.807162</td>\n",
       "      <td>0.646675</td>\n",
       "      <td>-1.024808</td>\n",
       "      <td>0.674876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.310494</td>\n",
       "      <td>-0.761696</td>\n",
       "      <td>1.510195</td>\n",
       "      <td>-1.091137</td>\n",
       "      <td>-0.564876</td>\n",
       "      <td>0.335877</td>\n",
       "      <td>-1.216107</td>\n",
       "      <td>0.807162</td>\n",
       "      <td>0.646675</td>\n",
       "      <td>-1.024808</td>\n",
       "      <td>-0.735362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Surname  CreditScore  Geography    Gender       Age    Tenure   Balance  \\\n",
       "0  1.361279     0.336963  -0.900813  0.916475 -0.086218  0.680987  0.534294   \n",
       "1  1.165571     0.616810  -0.900813 -1.091137 -0.277681 -0.354341  0.642170   \n",
       "2  0.129260     0.482069  -0.900813 -1.091137 -0.564876 -1.389668  0.182146   \n",
       "3 -0.739638     0.834469   0.304691  0.916475 -0.373413 -1.044559  0.830709   \n",
       "4 -0.310494    -0.761696   1.510195 -1.091137 -0.564876  0.335877 -1.216107   \n",
       "\n",
       "   NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0       0.807162   0.646675       -1.024808         0.041673  \n",
       "1      -0.913407   0.646675       -1.024808        -0.724481  \n",
       "2      -0.913407   0.646675       -1.024808         1.256317  \n",
       "3       0.807162   0.646675       -1.024808         0.674876  \n",
       "4       0.807162   0.646675       -1.024808        -0.735362  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of features after scaling:\n",
      " Surname           -5.329071e-18\n",
      "CreditScore        4.796163e-17\n",
      "Geography          8.171241e-17\n",
      "Gender             1.141309e-16\n",
      "Age                1.958433e-16\n",
      "Tenure             5.995204e-17\n",
      "Balance           -1.740830e-16\n",
      "NumOfProducts      2.220446e-18\n",
      "HasCrCard         -7.105427e-18\n",
      "IsActiveMember     1.096900e-16\n",
      "EstimatedSalary   -6.039613e-17\n",
      "dtype: float64\n",
      "\n",
      "Standard deviation of features after scaling:\n",
      " Surname            1.000063\n",
      "CreditScore        1.000063\n",
      "Geography          1.000063\n",
      "Gender             1.000063\n",
      "Age                1.000063\n",
      "Tenure             1.000063\n",
      "Balance            1.000063\n",
      "NumOfProducts      1.000063\n",
      "HasCrCard          1.000063\n",
      "IsActiveMember     1.000063\n",
      "EstimatedSalary    1.000063\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train_df = pd.DataFrame(X_train, columns=X.columns)\n",
    "\n",
    "print(\"Sample of normalized training data:\")\n",
    "display(X_train_df.head())\n",
    "\n",
    "print(\"\\nMean of features after scaling:\\n\", X_train_df.mean())\n",
    "print(\"\\nStandard deviation of features after scaling:\\n\", X_train_df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "158bd288-60d3-46ac-950f-53567af7a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "abacc6e2-4b11-4bda-806d-b693fe67b09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Dense in module keras.src.layers.core.dense:\n",
      "\n",
      "class Dense(keras.src.layers.layer.Layer)\n",
      " |  Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, lora_rank=None, lora_alpha=None, **kwargs)\n",
      " |  \n",
      " |  Just your regular densely-connected NN layer.\n",
      " |  \n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, kernel) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`).\n",
      " |  \n",
      " |  Note: If the input to the layer has a rank greater than 2, `Dense`\n",
      " |  computes the dot product between the `inputs` and the `kernel` along the\n",
      " |  last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).\n",
      " |  For example, if input has dimensions `(batch_size, d0, d1)`, then we create\n",
      " |  a `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2\n",
      " |  of the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are\n",
      " |  `batch_size * d0` such sub-tensors). The output in this case will have\n",
      " |  shape `(batch_size, d0, units)`.\n",
      " |  \n",
      " |  Args:\n",
      " |      units: Positive integer, dimensionality of the output space.\n",
      " |      activation: Activation function to use.\n",
      " |          If you don't specify anything, no activation is applied\n",
      " |          (ie. \"linear\" activation: `a(x) = x`).\n",
      " |      use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |      kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      " |      bias_initializer: Initializer for the bias vector.\n",
      " |      kernel_regularizer: Regularizer function applied to\n",
      " |          the `kernel` weights matrix.\n",
      " |      bias_regularizer: Regularizer function applied to the bias vector.\n",
      " |      activity_regularizer: Regularizer function applied to\n",
      " |          the output of the layer (its \"activation\").\n",
      " |      kernel_constraint: Constraint function applied to\n",
      " |          the `kernel` weights matrix.\n",
      " |      bias_constraint: Constraint function applied to the bias vector.\n",
      " |      lora_rank: Optional integer. If set, the layer's forward pass\n",
      " |          will implement LoRA (Low-Rank Adaptation)\n",
      " |          with the provided rank. LoRA sets the layer's kernel\n",
      " |          to non-trainable and replaces it with a delta over the\n",
      " |          original kernel, obtained via multiplying two lower-rank\n",
      " |          trainable matrices. This can be useful to reduce the\n",
      " |          computation cost of fine-tuning large dense layers.\n",
      " |          You can also enable LoRA on an existing\n",
      " |          `Dense` layer by calling `layer.enable_lora(rank)`.\n",
      " |      lora_alpha: Optional integer. If set, this parameter scales the\n",
      " |          low-rank adaptation delta (computed as the product of two lower-rank\n",
      " |          trainable matrices) during the forward pass. The delta is scaled by\n",
      " |          `lora_alpha / lora_rank`, allowing you to fine-tune the strength of\n",
      " |          the LoRA adjustment independently of `lora_rank`.\n",
      " |  \n",
      " |  Input shape:\n",
      " |      N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      " |      The most common situation would be\n",
      " |      a 2D input with shape `(batch_size, input_dim)`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |      N-D tensor with shape: `(batch_size, ..., units)`.\n",
      " |      For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      " |      the output would have shape `(batch_size, units)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      keras.src.layers.layer.Layer\n",
      " |      keras.src.backend.tensorflow.layer.TFLayer\n",
      " |      keras.src.backend.tensorflow.trackable.KerasAutoTrackable\n",
      " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
      " |      tensorflow.python.trackable.base.Trackable\n",
      " |      keras.src.ops.operation.Operation\n",
      " |      keras.src.saving.keras_saveable.KerasSaveable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, lora_rank=None, lora_alpha=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |  \n",
      " |  call(self, inputs, training=None)\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |  \n",
      " |  enable_lora(self, rank, lora_alpha=None, a_initializer='he_uniform', b_initializer='zeros')\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the object.\n",
      " |      \n",
      " |      An object config is a Python dictionary (serializable)\n",
      " |      containing the information needed to re-instantiate it.\n",
      " |  \n",
      " |  load_own_variables(self, store)\n",
      " |      Loads the state of the layer.\n",
      " |      \n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is loaded upon calling `keras.models.load_model()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          store: Dict from which the state of the model will be loaded.\n",
      " |  \n",
      " |  quantize(self, mode, type_check=True, config=None)\n",
      " |  \n",
      " |  quantized_build(self, kernel_shape, mode, config=None)\n",
      " |  \n",
      " |  save_own_variables(self, store)\n",
      " |      Saves the state of the layer.\n",
      " |      \n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is saved upon calling `model.save()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          store: Dict where the state of the model will be saved.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  kernel\n",
      " |  \n",
      " |  variable_serialization_spec\n",
      " |      Returns a dict mapping quantization modes to variable names in order.\n",
      " |      \n",
      " |      This spec is used by `save_own_variables` and `load_own_variables` to\n",
      " |      determine the correct ordering of variables during serialization for\n",
      " |      each quantization mode. `None` means no quantization.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.layers.layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_loss(self, loss)\n",
      " |      Can be called inside of the `call()` method to add a scalar loss.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(Layer):\n",
      " |          ...\n",
      " |          def call(self, x):\n",
      " |              self.add_loss(ops.sum(x))\n",
      " |              return x\n",
      " |      ```\n",
      " |  \n",
      " |  add_metric(self, *args, **kwargs)\n",
      " |  \n",
      " |  add_variable(self, shape, initializer, dtype=None, trainable=True, autocast=True, regularizer=None, constraint=None, name=None)\n",
      " |      Add a weight variable to the layer.\n",
      " |      \n",
      " |      Alias of `add_weight()`.\n",
      " |  \n",
      " |  add_weight(self, shape=None, initializer=None, dtype=None, trainable=True, autocast=True, regularizer=None, constraint=None, aggregation='none', overwrite_with_gradient=False, name=None)\n",
      " |      Add a weight variable to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |          shape: Shape tuple for the variable. Must be fully-defined\n",
      " |              (no `None` entries). Defaults to `()` (scalar) if unspecified.\n",
      " |          initializer: Initializer object to use to populate the initial\n",
      " |              variable value, or string name of a built-in initializer\n",
      " |              (e.g. `\"random_normal\"`). If unspecified, defaults to\n",
      " |              `\"glorot_uniform\"` for floating-point variables and to `\"zeros\"`\n",
      " |              for all other types (e.g. int, bool).\n",
      " |          dtype: Dtype of the variable to create, e.g. `\"float32\"`. If\n",
      " |              unspecified, defaults to the layer's variable dtype\n",
      " |              (which itself defaults to `\"float32\"` if unspecified).\n",
      " |          trainable: Boolean, whether the variable should be trainable via\n",
      " |              backprop or whether its updates are managed manually. Defaults\n",
      " |              to `True`.\n",
      " |          autocast: Boolean, whether to autocast layers variables when\n",
      " |              accessing them. Defaults to `True`.\n",
      " |          regularizer: Regularizer object to call to apply penalty on the\n",
      " |              weight. These penalties are summed into the loss function\n",
      " |              during optimization. Defaults to `None`.\n",
      " |          constraint: Contrainst object to call on the variable after any\n",
      " |              optimizer update, or string name of a built-in constraint.\n",
      " |              Defaults to `None`.\n",
      " |          aggregation: Optional string, one of `None`, `\"none\"`, `\"mean\"`,\n",
      " |              `\"sum\"` or `\"only_first_replica\"`. Annotates the variable with\n",
      " |              the type of multi-replica aggregation to be used for this\n",
      " |              variable when writing custom data parallel training loops.\n",
      " |              Defaults to `\"none\"`.\n",
      " |          overwrite_with_gradient: Boolean, whether to overwrite the variable\n",
      " |              with the computed gradient. This is useful for float8 training.\n",
      " |              Defaults to `False`.\n",
      " |          name: String name of the variable. Useful for debugging purposes.\n",
      " |  \n",
      " |  build_from_config(self, config)\n",
      " |      Builds the layer's states with the supplied config dict.\n",
      " |      \n",
      " |      By default, this method calls the `build(config[\"input_shape\"])` method,\n",
      " |      which creates weights based on the layer's input shape in the supplied\n",
      " |      config. If your config contains other information needed to load the\n",
      " |      layer's state, you should override this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: Dict containing the input shape associated with this layer.\n",
      " |  \n",
      " |  compute_mask(self, inputs, previous_mask)\n",
      " |  \n",
      " |  compute_output_spec(self, *args, **kwargs)\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |  \n",
      " |  get_build_config(self)\n",
      " |      Returns a dictionary with the layer's input shape.\n",
      " |      \n",
      " |      This method returns a config dict that can be used by\n",
      " |      `build_from_config(config)` to create all states (e.g. Variables and\n",
      " |      Lookup tables) needed by the layer.\n",
      " |      \n",
      " |      By default, the config only contains the input shape that the layer\n",
      " |      was built with. If you're writing a custom layer that creates state in\n",
      " |      an unusual way, you should override this method to make sure this state\n",
      " |      is already created when Keras attempts to load its value upon model\n",
      " |      loading.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict containing the input shape associated with the layer.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Return the values of `layer.weights` as a list of NumPy arrays.\n",
      " |  \n",
      " |  quantized_call(self, *args, **kwargs)\n",
      " |  \n",
      " |  rematerialized_call(self, layer_call, *args, **kwargs)\n",
      " |      Enable rematerialization dynamically for layer's call method.\n",
      " |      \n",
      " |      Args:\n",
      " |          layer_call: The original `call` method of a layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Rematerialized layer's `call` method.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the values of `layer.weights` from a list of NumPy arrays.\n",
      " |  \n",
      " |  stateless_call(self, trainable_variables, non_trainable_variables, *args, return_losses=False, **kwargs)\n",
      " |      Call the layer without any side effects.\n",
      " |      \n",
      " |      Args:\n",
      " |          trainable_variables: List of trainable variables of the model.\n",
      " |          non_trainable_variables: List of non-trainable variables of the\n",
      " |              model.\n",
      " |          *args: Positional arguments to be passed to `call()`.\n",
      " |          return_losses: If `True`, `stateless_call()` will return the list of\n",
      " |              losses created during `call()` as part of its return values.\n",
      " |          **kwargs: Keyword arguments to be passed to `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple. By default, returns `(outputs, non_trainable_variables)`.\n",
      " |              If `return_losses = True`, then returns\n",
      " |              `(outputs, non_trainable_variables, losses)`.\n",
      " |      \n",
      " |      Note: `non_trainable_variables` include not only non-trainable weights\n",
      " |      such as `BatchNormalization` statistics, but also RNG seed state\n",
      " |      (if there are any random operations part of the layer, such as dropout),\n",
      " |      and `Metric` state (if there are any metrics attached to the layer).\n",
      " |      These are all elements of state of the layer.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      model = ...\n",
      " |      data = ...\n",
      " |      trainable_variables = model.trainable_variables\n",
      " |      non_trainable_variables = model.non_trainable_variables\n",
      " |      # Call the model with zero side effects\n",
      " |      outputs, non_trainable_variables = model.stateless_call(\n",
      " |          trainable_variables,\n",
      " |          non_trainable_variables,\n",
      " |          data,\n",
      " |      )\n",
      " |      # Attach the updated state to the model\n",
      " |      # (until you do this, the model is still in its pre-call state).\n",
      " |      for ref_var, value in zip(\n",
      " |          model.non_trainable_variables, non_trainable_variables\n",
      " |      ):\n",
      " |          ref_var.assign(value)\n",
      " |      ```\n",
      " |  \n",
      " |  symbolic_call(self, *args, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from keras.src.layers.layer.Layer:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.layers.layer.Layer:\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the computations performed by the layer.\n",
      " |  \n",
      " |  dtype\n",
      " |      Alias of `layer.variable_dtype`.\n",
      " |  \n",
      " |  input_dtype\n",
      " |      The dtype layer inputs should be converted to.\n",
      " |  \n",
      " |  losses\n",
      " |      List of scalar losses from `add_loss`, regularizers and sublayers.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of all metrics.\n",
      " |  \n",
      " |  metrics_variables\n",
      " |      List of all metric variables.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |      List of all non-trainable layer state.\n",
      " |      \n",
      " |      This extends `layer.non_trainable_weights` to include all state used by\n",
      " |      the layer including state for metrics and `SeedGenerator`s.\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weight variables of the layer.\n",
      " |      \n",
      " |      These are the weights that should not be updated by the optimizer during\n",
      " |      training. Unlike, `layer.non_trainable_variables` this excludes metric\n",
      " |      state and random seeds.\n",
      " |  \n",
      " |  path\n",
      " |      The path of the layer.\n",
      " |      \n",
      " |      If the layer has not been built yet, it will be `None`.\n",
      " |  \n",
      " |  quantization_mode\n",
      " |      The quantization mode of this layer, `None` if not quantized.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      List of all trainable layer state.\n",
      " |      \n",
      " |      This is equivalent to `layer.trainable_weights`.\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weight variables of the layer.\n",
      " |      \n",
      " |      These are the weights that get updated by the optimizer during training.\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      The dtype of the state (weights) of the layer.\n",
      " |  \n",
      " |  variables\n",
      " |      List of all layer state, including random seeds.\n",
      " |      \n",
      " |      This extends `layer.weights` to include all state used by the layer\n",
      " |      including `SeedGenerator`s.\n",
      " |      \n",
      " |      Note that metrics variables are not included here, use\n",
      " |      `metrics_variables` to visit all the metric variables.\n",
      " |  \n",
      " |  weights\n",
      " |      List of all weight variables of the layer.\n",
      " |      \n",
      " |      Unlike, `layer.variables` this excludes metric state and random seeds.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.layers.layer.Layer:\n",
      " |  \n",
      " |  dtype_policy\n",
      " |  \n",
      " |  input_spec\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |      Settable boolean, whether this layer should be trainable or not.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.trackable.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.src.ops.operation.Operation:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates an operation from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`, capable of instantiating the\n",
      " |      same operation from the config dictionary.\n",
      " |      \n",
      " |      Note: If you override this method, you might receive a serialized dtype\n",
      " |      config, which is a `dict`. You can deserialize it as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      if \"dtype\" in config and isinstance(config[\"dtype\"], dict):\n",
      " |          policy = dtype_policies.deserialize(config[\"dtype\"])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the output of `get_config`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An operation instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.ops.operation.Operation:\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a symbolic operation.\n",
      " |      \n",
      " |      Only returns the tensor(s) corresponding to the *first time*\n",
      " |      the operation was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only returns the tensor(s) corresponding to the *first time*\n",
      " |      the operation was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output tensor or list of output tensors.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.saving.keras_saveable.KerasSaveable:\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      __reduce__ is used to customize the behavior of `pickle.pickle()`.\n",
      " |      \n",
      " |      The method returns a tuple of two elements: a function, and a list of\n",
      " |      arguments to pass to that function.  In this case we just leverage the\n",
      " |      keras saving library.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7d4846b5-2982-486c-bc4c-79d04b857f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=20, activation='relu', input_dim = X_train.shape[1]))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=80, activation = 'relu'))\n",
    "model.add(Dense(units=300, activation = 'relu'))\n",
    "model.add(Dense(units=300, activation = 'relu'))\n",
    "\n",
    "model.add(Dense(units=500, activation = 'relu'))\n",
    "model.add(Dense(units=300, activation = 'relu'))\n",
    "model.add(Dense(units=300, activation = 'relu'))\n",
    "model.add(Dense(units=100, activation = 'relu'))\n",
    "model.add(Dense(units=20, activation = 'relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "model.add(Dense(units=5, activation = 'relu'))\n",
    "model.add(Dense(units=1, activation = 'sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "71717ced-937c-45d0-91f0-c1e2c83b497a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method compile in module keras.src.trainers.trainer:\n",
      "\n",
      "compile(optimizer='rmsprop', loss=None, loss_weights=None, metrics=None, weighted_metrics=None, run_eagerly=False, steps_per_execution=1, jit_compile='auto', auto_scale_loss=True) method of keras.src.models.sequential.Sequential instance\n",
      "    Configures the model for training.\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    ```python\n",
      "    model.compile(\n",
      "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
      "        loss=keras.losses.BinaryCrossentropy(),\n",
      "        metrics=[\n",
      "            keras.metrics.BinaryAccuracy(),\n",
      "            keras.metrics.FalseNegatives(),\n",
      "        ],\n",
      "    )\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "        optimizer: String (name of optimizer) or optimizer instance. See\n",
      "            `keras.optimizers`.\n",
      "        loss: Loss function. May be a string (name of loss function), or\n",
      "            a `keras.losses.Loss` instance. See `keras.losses`. A\n",
      "            loss function is any callable with the signature\n",
      "            `loss = fn(y_true, y_pred)`, where `y_true` are the ground truth\n",
      "            values, and `y_pred` are the model's predictions.\n",
      "            `y_true` should have shape `(batch_size, d0, .. dN)`\n",
      "            (except in the case of sparse loss functions such as\n",
      "            sparse categorical crossentropy which expects integer arrays of\n",
      "            shape `(batch_size, d0, .. dN-1)`).\n",
      "            `y_pred` should have shape `(batch_size, d0, .. dN)`.\n",
      "            The loss function should return a float tensor.\n",
      "        loss_weights: Optional list or dictionary specifying scalar\n",
      "            coefficients (Python floats) to weight the loss contributions of\n",
      "            different model outputs. The loss value that will be minimized\n",
      "            by the model will then be the *weighted sum* of all individual\n",
      "            losses, weighted by the `loss_weights` coefficients.  If a list,\n",
      "            it is expected to have a 1:1 mapping to the model's outputs. If\n",
      "            a dict, it is expected to map output names (strings) to scalar\n",
      "            coefficients.\n",
      "        metrics: List of metrics to be evaluated by the model during\n",
      "            training and testing. Each of this can be a string (name of a\n",
      "            built-in function), function or a `keras.metrics.Metric`\n",
      "            instance. See `keras.metrics`. Typically you will use\n",
      "            `metrics=['accuracy']`. A function is any callable with the\n",
      "            signature `result = fn(y_true, _pred)`. To specify different\n",
      "            metrics for different outputs of a multi-output model, you could\n",
      "            also pass a dictionary, such as\n",
      "            `metrics={'a':'accuracy', 'b':['accuracy', 'mse']}`.\n",
      "            You can also pass a list to specify a metric or a list of\n",
      "            metrics for each output, such as\n",
      "            `metrics=[['accuracy'], ['accuracy', 'mse']]`\n",
      "            or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass\n",
      "            the strings 'accuracy' or 'acc', we convert this to one of\n",
      "            `keras.metrics.BinaryAccuracy`,\n",
      "            `keras.metrics.CategoricalAccuracy`,\n",
      "            `keras.metrics.SparseCategoricalAccuracy` based on the\n",
      "            shapes of the targets and of the model output. A similar\n",
      "            conversion is done for the strings `\"crossentropy\"`\n",
      "            and `\"ce\"` as well.\n",
      "            The metrics passed here are evaluated without sample weighting;\n",
      "            if you would like sample weighting to apply, you can specify\n",
      "            your metrics via the `weighted_metrics` argument instead.\n",
      "        weighted_metrics: List of metrics to be evaluated and weighted by\n",
      "            `sample_weight` or `class_weight` during training and testing.\n",
      "        run_eagerly: Bool. If `True`, this model's forward pass\n",
      "             will never be compiled. It is recommended to leave this\n",
      "             as `False` when training (for best performance),\n",
      "             and to set it to `True` when debugging.\n",
      "        steps_per_execution: Int. The number of batches to run\n",
      "            during each a single compiled function call. Running multiple\n",
      "            batches inside a single compiled function call can\n",
      "            greatly improve performance on TPUs or small models with a large\n",
      "            Python overhead. At most, one full epoch will be run each\n",
      "            execution. If a number larger than the size of the epoch is\n",
      "            passed, the execution will be truncated to the size of the\n",
      "            epoch. Note that if `steps_per_execution` is set to `N`,\n",
      "            `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n",
      "            will only be called every `N` batches (i.e. before/after\n",
      "            each compiled function execution).\n",
      "            Not supported with the PyTorch backend.\n",
      "        jit_compile: Bool or `\"auto\"`. Whether to use XLA compilation when\n",
      "            compiling a model. For `jax` and `tensorflow` backends,\n",
      "            `jit_compile=\"auto\"` enables XLA compilation if the model\n",
      "            supports it, and disabled otherwise.\n",
      "            For `torch` backend, `\"auto\"` will default to eager\n",
      "            execution and `jit_compile=True` will run with `torch.compile`\n",
      "            with the `\"inductor\"` backend.\n",
      "        auto_scale_loss: Bool. If `True` and the model dtype policy is\n",
      "            `\"mixed_float16\"`, the passed optimizer will be automatically\n",
      "            wrapped in a `LossScaleOptimizer`, which will dynamically\n",
      "            scale the loss to prevent underflow.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "680f6670-a981-44d6-a644-a5b4ffec35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7576f50d-c84a-4388-9517-95025d9fc867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module keras.src.backend.tensorflow.trainer:\n",
      "\n",
      "fit(x=None, y=None, batch_size=None, epochs=1, verbose='auto', callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1) method of keras.src.models.sequential.Sequential instance\n",
      "    Trains the model for a fixed number of epochs (dataset iterations).\n",
      "    \n",
      "    Args:\n",
      "        x: Input data. It can be:\n",
      "            - A NumPy array (or array-like), or a list of arrays\n",
      "            (in case the model has multiple inputs).\n",
      "            - A backend-native tensor, or a list of tensors\n",
      "            (in case the model has multiple inputs).\n",
      "            - A dict mapping input names to the corresponding array/tensors,\n",
      "            if the model has named inputs.\n",
      "            - A `keras.utils.PyDataset` returning `(inputs, targets)` or\n",
      "            `(inputs, targets, sample_weights)`.\n",
      "            - A `tf.data.Dataset` yielding `(inputs, targets)` or\n",
      "            `(inputs, targets, sample_weights)`.\n",
      "            - A `torch.utils.data.DataLoader` yielding `(inputs, targets)`\n",
      "            or `(inputs, targets, sample_weights)`.\n",
      "            - A Python generator function yielding `(inputs, targets)` or\n",
      "            `(inputs, targets, sample_weights)`.\n",
      "        y: Target data. Like the input data `x`, it can be either NumPy\n",
      "            array(s) or backend-native tensor(s). If `x` is a\n",
      "            `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
      "            `torch.utils.data.DataLoader` or a Python generator function,\n",
      "            `y` should not be specified since targets will be obtained from\n",
      "            `x`.\n",
      "        batch_size: Integer or `None`.\n",
      "            Number of samples per gradient update.\n",
      "            If unspecified, `batch_size` will default to 32.\n",
      "            Do not specify the `batch_size` if your input data `x` is a\n",
      "            `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
      "            `torch.utils.data.DataLoader` or Python generator function\n",
      "            since they generate batches.\n",
      "        epochs: Integer. Number of epochs to train the model.\n",
      "            An epoch is an iteration over the entire `x` and `y`\n",
      "            data provided\n",
      "            (unless the `steps_per_epoch` flag is set to\n",
      "            something other than None).\n",
      "            Note that in conjunction with `initial_epoch`,\n",
      "            `epochs` is to be understood as \"final epoch\".\n",
      "            The model is not trained for a number of iterations\n",
      "            given by `epochs`, but merely until the epoch\n",
      "            of index `epochs` is reached.\n",
      "        verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n",
      "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "            \"auto\" becomes 1 for most cases.\n",
      "            Note that the progress bar is not\n",
      "            particularly useful when logged to a file,\n",
      "            so `verbose=2` is recommended when not running interactively\n",
      "            (e.g., in a production environment). Defaults to `\"auto\"`.\n",
      "        callbacks: List of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training.\n",
      "            See `keras.callbacks`. Note\n",
      "            `keras.callbacks.ProgbarLogger` and\n",
      "            `keras.callbacks.History` callbacks are created\n",
      "            automatically and need not be passed to `model.fit()`.\n",
      "            `keras.callbacks.ProgbarLogger` is created\n",
      "            or not based on the `verbose` argument in `model.fit()`.\n",
      "        validation_split: Float between 0 and 1.\n",
      "            Fraction of the training data to be used as validation data.\n",
      "            The model will set apart this fraction of the training data,\n",
      "            will not train on it, and will evaluate the loss and any model\n",
      "            metrics on this data at the end of each epoch. The validation\n",
      "            data is selected from the last samples in the `x` and `y` data\n",
      "            provided, before shuffling.\n",
      "            This argument is only supported when `x` and `y` are made of\n",
      "            NumPy arrays or tensors.\n",
      "            If both `validation_data` and `validation_split` are provided,\n",
      "            `validation_data` will override `validation_split`.\n",
      "        validation_data: Data on which to evaluate\n",
      "            the loss and any model metrics at the end of each epoch.\n",
      "            The model will not be trained on this data. Thus, note the fact\n",
      "            that the validation loss of data provided using\n",
      "            `validation_split` or `validation_data` is not affected by\n",
      "            regularization layers like noise and dropout.\n",
      "            `validation_data` will override `validation_split`.\n",
      "            It can be:\n",
      "            - A tuple `(x_val, y_val)` of NumPy arrays or tensors.\n",
      "            - A tuple `(x_val, y_val, val_sample_weights)` of NumPy\n",
      "            arrays.\n",
      "            - A `keras.utils.PyDataset`, a `tf.data.Dataset`, a\n",
      "            `torch.utils.data.DataLoader` yielding `(inputs, targets)` or a\n",
      "            Python generator function yielding `(x_val, y_val)` or\n",
      "            `(inputs, targets, sample_weights)`.\n",
      "        shuffle: Boolean, whether to shuffle the training data before each\n",
      "            epoch. This argument is ignored when `x` is a\n",
      "            `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
      "            `torch.utils.data.DataLoader` or Python generator function.\n",
      "        class_weight: Optional dictionary mapping class indices (integers)\n",
      "            to a weight (float) value, used for weighting the loss function\n",
      "            (during training only).\n",
      "            This can be useful to tell the model to\n",
      "            \"pay more attention\" to samples from\n",
      "            an under-represented class. When `class_weight` is specified\n",
      "            and targets have a rank of 2 or greater, either `y` must be\n",
      "            one-hot encoded, or an explicit final dimension of `1` must\n",
      "            be included for sparse class labels.\n",
      "        sample_weight: Optional NumPy array or tensor of weights for\n",
      "            the training samples, used for weighting the loss function\n",
      "            (during training only). You can either pass a flat (1D)\n",
      "            NumPy array or tensor with the same length as the input samples\n",
      "            (1:1 mapping between weights and samples), or in the case of\n",
      "            temporal data, you can pass a 2D NumPy array or tensor with\n",
      "            shape `(samples, sequence_length)` to apply a different weight\n",
      "            to every timestep of every sample.\n",
      "            This argument is not supported when `x` is a\n",
      "            `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
      "            `torch.utils.data.DataLoader` or Python generator function.\n",
      "            Instead, provide `sample_weights` as the third element of `x`.\n",
      "            Note that sample weighting does not apply to metrics specified\n",
      "            via the `metrics` argument in `compile()`. To apply sample\n",
      "            weighting to your metrics, you can specify them via the\n",
      "            `weighted_metrics` in `compile()` instead.\n",
      "        initial_epoch: Integer.\n",
      "            Epoch at which to start training\n",
      "            (useful for resuming a previous training run).\n",
      "        steps_per_epoch: Integer or `None`.\n",
      "            Total number of steps (batches of samples) before declaring one\n",
      "            epoch finished and starting the next epoch. When training with\n",
      "            input tensors or NumPy arrays, the default `None` means that the\n",
      "            value used is the number of samples in your dataset divided by\n",
      "            the batch size, or 1 if that cannot be determined.\n",
      "            If `x` is a `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
      "            `torch.utils.data.DataLoader` or Python generator function, the\n",
      "            epoch will run until the input dataset is exhausted. When\n",
      "            passing an infinitely repeating dataset, you must specify the\n",
      "            `steps_per_epoch` argument, otherwise the training will run\n",
      "            indefinitely.\n",
      "        validation_steps: Integer or `None`.\n",
      "            Only relevant if `validation_data` is provided.\n",
      "            Total number of steps (batches of samples) to draw before\n",
      "            stopping when performing validation at the end of every epoch.\n",
      "            If `validation_steps` is `None`, validation will run until the\n",
      "            `validation_data` dataset is exhausted. In the case of an\n",
      "            infinitely repeating dataset, it will run indefinitely. If\n",
      "            `validation_steps` is specified and only part of the dataset\n",
      "            is consumed, the evaluation will start from the beginning of the\n",
      "            dataset at each epoch. This ensures that the same validation\n",
      "            samples are used every time.\n",
      "        validation_batch_size: Integer or `None`.\n",
      "            Number of samples per validation batch.\n",
      "            If unspecified, will default to `batch_size`.\n",
      "            Do not specify the `validation_batch_size` if your data is a\n",
      "            `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
      "            `torch.utils.data.DataLoader` or Python generator function\n",
      "            since they generate batches.\n",
      "        validation_freq: Only relevant if validation data is provided.\n",
      "            Specifies how many training epochs to run\n",
      "            before a new validation run is performed,\n",
      "            e.g. `validation_freq=2` runs validation every 2 epochs.\n",
      "    \n",
      "    Unpacking behavior for iterator-like inputs:\n",
      "        A common pattern is to pass an iterator like object such as a\n",
      "        `tf.data.Dataset` or a `keras.utils.PyDataset` to `fit()`,\n",
      "        which will in fact yield not only features (`x`)\n",
      "        but optionally targets (`y`) and sample weights (`sample_weight`).\n",
      "        Keras requires that the output of such iterator-likes be\n",
      "        unambiguous. The iterator should return a tuple\n",
      "        of length 1, 2, or 3, where the optional second and third elements\n",
      "        will be used for `y` and `sample_weight` respectively.\n",
      "        Any other type provided will be wrapped in\n",
      "        a length-one tuple, effectively treating everything as `x`. When\n",
      "        yielding dicts, they should still adhere to the top-level tuple\n",
      "        structure,\n",
      "        e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
      "        features, targets, and weights from the keys of a single dict.\n",
      "        A notable unsupported data type is the `namedtuple`. The reason is\n",
      "        that it behaves like both an ordered datatype (tuple) and a mapping\n",
      "        datatype (dict). So given a namedtuple of the form:\n",
      "        `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
      "        it is ambiguous whether to reverse the order of the elements when\n",
      "        interpreting the value. Even worse is a tuple of the form:\n",
      "        `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
      "        where it is unclear if the tuple was intended to be unpacked\n",
      "        into `x`, `y`, and `sample_weight` or passed through\n",
      "        as a single element to `x`.\n",
      "    \n",
      "    Returns:\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4a23a738-3aff-486f-bbf3-f2498a478010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.7958 - loss: 0.5748 - val_accuracy: 0.7985 - val_loss: 0.5201\n",
      "Epoch 2/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7958 - loss: 0.5029 - val_accuracy: 0.7985 - val_loss: 0.4805\n",
      "Epoch 3/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7958 - loss: 0.4740 - val_accuracy: 0.7985 - val_loss: 0.4440\n",
      "Epoch 4/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7958 - loss: 0.4601 - val_accuracy: 0.7985 - val_loss: 0.4314\n",
      "Epoch 5/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7958 - loss: 0.4517 - val_accuracy: 0.7985 - val_loss: 0.4256\n",
      "Epoch 6/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7958 - loss: 0.4405 - val_accuracy: 0.7985 - val_loss: 0.4202\n",
      "Epoch 7/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7958 - loss: 0.4391 - val_accuracy: 0.7985 - val_loss: 0.4212\n",
      "Epoch 8/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7958 - loss: 0.4310 - val_accuracy: 0.7985 - val_loss: 0.4134\n",
      "Epoch 9/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7958 - loss: 0.4335 - val_accuracy: 0.7985 - val_loss: 0.4179\n",
      "Epoch 10/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7958 - loss: 0.4289 - val_accuracy: 0.7985 - val_loss: 0.4144\n",
      "Epoch 11/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7958 - loss: 0.4226 - val_accuracy: 0.7985 - val_loss: 0.4110\n",
      "Epoch 12/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7958 - loss: 0.4224 - val_accuracy: 0.7985 - val_loss: 0.4109\n",
      "Epoch 13/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7958 - loss: 0.4204 - val_accuracy: 0.7985 - val_loss: 0.4079\n",
      "Epoch 14/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8016 - loss: 0.4168 - val_accuracy: 0.8245 - val_loss: 0.4054\n",
      "Epoch 15/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8279 - loss: 0.4128 - val_accuracy: 0.8310 - val_loss: 0.4066\n",
      "Epoch 16/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8255 - loss: 0.4183 - val_accuracy: 0.8385 - val_loss: 0.4065\n",
      "Epoch 17/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8301 - loss: 0.4116 - val_accuracy: 0.8435 - val_loss: 0.4028\n",
      "Epoch 18/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8319 - loss: 0.4061 - val_accuracy: 0.8390 - val_loss: 0.4012\n",
      "Epoch 19/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8304 - loss: 0.4096 - val_accuracy: 0.8415 - val_loss: 0.4035\n",
      "Epoch 20/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8320 - loss: 0.4051 - val_accuracy: 0.8340 - val_loss: 0.4028\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=1024, epochs=20, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9527699a-79f8-403d-b095-376e7ac794e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "be8ee1c5-24e0-403e-b7ee-6adfebfc818f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy per Epoch:\n",
      "\n",
      "Epoch 1: Training Accuracy = 76.56%, Validation Accuracy = 79.85%\n",
      "Epoch 2: Training Accuracy = 79.58%, Validation Accuracy = 79.85%\n",
      "Epoch 3: Training Accuracy = 79.58%, Validation Accuracy = 79.85%\n",
      "Epoch 4: Training Accuracy = 79.58%, Validation Accuracy = 79.85%\n",
      "Epoch 5: Training Accuracy = 79.58%, Validation Accuracy = 79.85%\n",
      "Epoch 6: Training Accuracy = 79.58%, Validation Accuracy = 79.85%\n",
      "Epoch 7: Training Accuracy = 80.18%, Validation Accuracy = 80.45%\n",
      "Epoch 8: Training Accuracy = 80.90%, Validation Accuracy = 81.80%\n",
      "Epoch 9: Training Accuracy = 81.40%, Validation Accuracy = 80.95%\n",
      "Epoch 10: Training Accuracy = 80.86%, Validation Accuracy = 82.30%\n",
      "Epoch 11: Training Accuracy = 81.40%, Validation Accuracy = 82.15%\n",
      "Epoch 12: Training Accuracy = 81.34%, Validation Accuracy = 83.00%\n",
      "Epoch 13: Training Accuracy = 82.30%, Validation Accuracy = 83.35%\n",
      "Epoch 14: Training Accuracy = 82.21%, Validation Accuracy = 83.60%\n",
      "Epoch 15: Training Accuracy = 82.91%, Validation Accuracy = 83.65%\n",
      "Epoch 16: Training Accuracy = 82.82%, Validation Accuracy = 83.65%\n",
      "Epoch 17: Training Accuracy = 83.07%, Validation Accuracy = 83.00%\n",
      "Epoch 18: Training Accuracy = 83.28%, Validation Accuracy = 83.05%\n",
      "Epoch 19: Training Accuracy = 83.10%, Validation Accuracy = 82.65%\n",
      "Epoch 20: Training Accuracy = 83.61%, Validation Accuracy = 83.15%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy per Epoch:\\n\")\n",
    "for i in range(len(train_acc)):\n",
    "    print(f\"Epoch {i+1}: Training Accuracy = {train_acc[i]*100:.2f}%, Validation Accuracy = {val_acc[i]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45f14ee9-5e07-400b-8314-1cd4ac1ca533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "10a84524-c48c-4ca4-890d-1d031cd72ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Accuracy per epoch\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2660a8b-7956-4b61-8389-a6f35eeaf943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
